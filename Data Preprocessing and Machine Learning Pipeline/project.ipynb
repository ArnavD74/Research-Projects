{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "First, we will model linear regression based on a data set that contains housing data (USA_Housing.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/srv/shared/USA_Housing.csv')\n",
    "if 'Address' in df.columns:\n",
    "    df_adjusted = df.drop(df.columns[-1], axis=1) #remove address col, fix col headers\n",
    "else:\n",
    "    df_adjusted = df\n",
    "df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_training, df_testing = train_test_split(df_adjusted, test_size=0.1, random_state=42)\n",
    "df_training = df_adjusted.drop(df_testing.index)\n",
    "df_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled_values = []\n",
    "X_scaled_values.append(scaler.fit_transform(df_training.iloc[:,[0]]))\n",
    "X_scaled_values.append(scaler.fit_transform(df_training.iloc[:,[1]]))\n",
    "X_scaled_values.append(scaler.fit_transform(df_training.iloc[:,[2]]))\n",
    "X_scaled_values.append(scaler.fit_transform(df_training.iloc[:,[3]]))\n",
    "X_scaled_values.append(scaler.fit_transform(df_training.iloc[:,[4]]))\n",
    "Y_scaled_values = scaler.fit_transform(df_training.iloc[:,[5]])\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[1], y, alpha = 0.5, s = 20)\n",
    "m, b = np.polyfit(x[1].flatten(), y, 1) #np.polyfit assigns to two vars\n",
    "plt.plot(x[1], m*x[1]+b, color ='red') #mx[i]+b fits linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[2], y, alpha = 0.5, s = 20)\n",
    "m, b = np.polyfit(x[2].flatten(), y, 1)\n",
    "plt.plot(x[2], m*x[2]+b, color ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[0], y, alpha = 0.5, s = 20)\n",
    "m, b = np.polyfit(x[0].flatten(), y, 1)\n",
    "plt.plot(x[0], m*x[0]+b, color ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[3], y, alpha = 0.5, s = 20)\n",
    "m, b = np.polyfit(x[3].flatten(), y, 1)\n",
    "plt.plot(x[3], m*x[3]+b, color ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[4], y, alpha = 0.5, s = 20)\n",
    "m, b = np.polyfit(x[4].flatten(), y, 1)\n",
    "plt.plot(x[4], m*x[4]+b, color ='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on my plots, income and the house age are good features for a linear regressional model as they seem to best fit a linear trend and have less variance than the other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Exploration of Linear Regression Line\n",
    "\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "to all data points (x,y), such that the L2 error \n",
    "$$\n",
    "E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. We need to manually change the values of theta0 and theta1 such that we obtain the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \"\"\"\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sqerror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model\n",
    "    Input: x, y vectors\n",
    "    Returns: average L2 square error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    res = ((h(theta0, theta1, x) - y)**2).mean()\n",
    "    return res\n",
    "print(sqerror(x[0], y, 0.29,0.52))\n",
    "print(sqerror(x[1], y, 0.29,0.52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def abserror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: average L1 error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    res = (abs(y - h(theta0, theta1, x))).mean()\n",
    "    return res\n",
    "print(abserror(x[0], y, 0.29,0.52))\n",
    "print(abserror(x[1], y, 0.29,0.52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def huberror(x, y, theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0, theta1 and delta of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: psuedo huber error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    a = abs(h(theta0, theta1, x) - y)\n",
    "    return np.mean(delta**2 * (np.sqrt(1 + (a/delta)**2) - 1))\n",
    "print(huberror(x[0], y, 0.29,0.52,0.1))\n",
    "print(huberror(x[1], y, 0.29,0.52,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    y1 = h(theta0, theta1, x[0]) \n",
    "    pylab.plot(x[0], y1) \n",
    "    sqerr = round(sqerror(x[0], y, theta0, theta1), 5)\n",
    "    abserr = round(abserror(x[0], y, theta0, theta1), 5)\n",
    "    huber = round(huberror(x[0], y, theta0, theta1, 0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huber))\n",
    "    x1 = x[0]   \n",
    "    y1 = Y_scaled_values\n",
    "    pylab.scatter(x1, y1, alpha = 0.5)\n",
    "    pylab.show() # show the plot  \n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = 0.5\n",
    "theta1 = 0.2\n",
    "error = 0.04266\n",
    "theta0 = 0.2\n",
    "theta1 = 0.4\n",
    "error =  0.1067\n",
    "theta0 = 0.29\n",
    "theta1 = 0.47\n",
    "error = 0.0089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Univariate\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the error function. The iterative formulas using L2 loss function for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x^j + \\theta_0)-y^j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x^j + \\theta_0 - y^j)*x^j\n",
    "$$\n",
    "\n",
    "The alpha is called the \"learning rate\". $(x^j, y^j)$ is the j-th observation. It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or be at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works. When implementing you need to simulataneously compute the values of theta0 and theta1. (do not use a changed theta0 value in computing theta1 in the same iteration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    iterations = 0\n",
    "    theta0 = 0\n",
    "    theta1 = 0\n",
    "    n0 = 0\n",
    "    n1 = 0\n",
    "    oldError = -1\n",
    "    newError = sqerror(obsX, obsY, n0, n1)\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        oldError = sqerror(obsX, obsY, n0, n1)\n",
    "        theta0 = n0\n",
    "        theta1 = n1\n",
    "        n0 = theta0 - alpha*float(sum(theta1*obsX+theta0-obsY) / len(obsX))\n",
    "        n1 = theta1 - alpha*float(sum((theta1*obsX+theta0-obsY)*obsX) / len(obsX))\n",
    "        newError = sqerror(obsX, obsY, n0, n1)\n",
    "        iterations += 1\n",
    "        print(theta0, theta1) #print out the theta0 and theta1 values for each iteration in your function\n",
    "    return n0, n1, newError, iterations\n",
    "[theta0,theta1,newError,iterations] = gd2(x[0],y,0.01,0.0001)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    iterations = 0\n",
    "    oldError = -1\n",
    "    newError = 0\n",
    "    theta0 = 0\n",
    "    theta1 = 0\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        oldError = huberror(obsX, obsY, theta0, theta1, delta)\n",
    "        val = h(theta0,theta1,obsX)-obsY\n",
    "        theta0 = theta0 - alpha*float(sum(val/(val**2/delta**2+1)**0.5)/len(obsX))\n",
    "        theta1 = theta1 - alpha*float(sum(obsX*val/(val**2/delta**2+1)**0.5)/len(obsX))\n",
    "        newError = huberror(obsX, obsY, theta0, theta1, delta)\n",
    "        iterations += 1\n",
    "        if(iterations % 10 == 0):\n",
    "            print(theta0, theta1) #print out the theta0 and theta1 values for each iteration in your function\n",
    "    return theta0, theta1, newError, iterations\n",
    "[theta0,theta1,newError,iterations] = gdh(x[0],y,0.01,0.000001,0.01)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "> theta0 = 0.23060509068378196\n",
    "\n",
    "> theta1 = 0.13141113134375276\n",
    "\n",
    "> alpha = 0.01\n",
    "\n",
    "> error = 0.002097\n",
    "\n",
    "2.\n",
    "\n",
    "> The widget shows the same or similar listings, and the huber error is similar to the errors found through the gdh function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(x[0],y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(x[0],y,theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extending the Model to a Bivariate\n",
    "In this task we extend the model to predict housing price using two features \"$x_1 = $Avg. Area House Age\" and \"$x_2 = $Avg. Area Number of Rooms\". The regression model is then defined by  \n",
    "$$\n",
    "y = \\theta_2*x_2 + \\theta_1*x_1 + \\theta_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd22(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1, theta2 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    iterations = 0\n",
    "    y = lambda x1, x2, t0, t1, t2: t2*x2+t1*x1+t0\n",
    "    theta0 = 0\n",
    "    theta1 = 0\n",
    "    theta2 = 0\n",
    "    oldError = -1\n",
    "    newError = 0\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        oldError = np.mean(np.square((theta2*obsX[1] + theta1*obsX[0] + theta0) - obsY))\n",
    "        gd = (theta2*obsX[1] + theta1*obsX[0] + theta0) - obsY\n",
    "        theta0 = theta0 - alpha*float(sum(gd) / len(obsX[0]))\n",
    "        theta1 = theta1 - alpha*float(sum(gd*obsX[0]) / len(obsX[0]))\n",
    "        theta2 = theta2 - alpha*float(sum(gd*obsX[1]) / len(obsX[0]))\n",
    "        newError = np.mean(np.square(((theta2*obsX[1]+ theta1*obsX[0] + theta0) - obsY)))\n",
    "        iterations += 1\n",
    "    return theta0, theta1, theta2, newError, iterations\n",
    "[theta0,theta1,theta2, newError,iterations] = gd22(x,y,0.01,0.0001)\n",
    "print(iterations)\n",
    "print(theta0, theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the values of thetas obtained from function above.\n",
    "$$\\theta_0 = 0.29191222024889424$$ \n",
    "$$\\theta_1 = 0.17880264543648708$$ \n",
    "$$\\theta_2 = 0.1514344524598622$$ \n",
    "and write the model \n",
    "$$\n",
    "y = 0.1514344524598622*x_2 + 0.17880264543648708*x_1 + 0.29191222024889424\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "y = df_training['Price']\n",
    "X = df_training[[\"Avg. Area House Age\", \"Avg. Area Number of Rooms\"]]\n",
    "model = LinearRegression().fit(X, y)\n",
    "model\n",
    "print(model.coef_)\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X0_scaled_values = scaler.fit_transform(df_testing.iloc[:,[1]])\n",
    "X1_scaled_values = scaler.fit_transform(df_testing.iloc[:,[2]])\n",
    "X_scaled_values = [X0_scaled_values,X1_scaled_values]\n",
    "Y_scaled_values = scaler.fit_transform(df_testing.iloc[:,[5]])\n",
    "res = 0.1514344524598622*X_scaled_values[1] + 0.17880264543648708*X_scaled_values[0] + 0.29191222024889424 #uses formula\n",
    "error_gd = float(sum(abs(res - Y_scaled_values)) / len(Y_scaled_values)) #uses avg\n",
    "print(error_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0.99178836*X_scaled_values[1] + 0.95831457 *X_scaled_values[0] + 0.6755587973 #uses formula\n",
    "error_lib = float(sum(abs(res - Y_scaled_values)) / len(Y_scaled_values)) # avg\n",
    "print(error_lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be similar, since their error values are similar one should not have more disparity over the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
