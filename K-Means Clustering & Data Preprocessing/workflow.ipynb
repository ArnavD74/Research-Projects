{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow: Data Processing & Model Training\n",
    "\n",
    "Any given color image can be considered a matrix of pixels where each pixel contains (R,G,B) values between 0-255. \n",
    "\n",
    "Each color is represented as an unsigned 8-bit integer. Grayscale images are represented by one byte per pixel. In this lab, we will learn how to read grayscale images into an ndarray or data frame and manipulate them to do some interesting operations on the image collection. We will use kmeans to cluster a set of images and understand how well they are being clustered into correct segments using k-means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Images\n",
    "Given a repository of images of handwritten digits, we can use unsupervised learning methods such as k-means to group the images into k clusters. Ideally with k=10, we should be able to group all images of 0,1,...,9 into 10 clusters. However, in practice this may not be possible as per reasons discussed in the lecture. In this part of the lab, we will look for the minimum k, that will provide us with the representation of all digits from 0 to 9. For the image data set, we will use the 1700+ low resolution 8x8 handwritten digit images from sklearn and apply k-means algorithm to find how well it can group/cluster into similar digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Defaults\n",
    "Load the digits from sklearn into an image array. Print the first 5 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.target)   # actual labels of the images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "for k in range(5):\n",
    "    #print(digits.target[k])\n",
    "    plt.matshow(digits.images[k])\n",
    "plt.show()\n",
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the 2D images\n",
    "The original images are 8x8 matrices of integers who values are between 0-255. Flatten them to represent images as dimension 64 vectors. We will use these vectors in the kmeans algorithm as our data vectors. Store each image as a row in the matrix called flatten_images. The flatten_images should be dimension n x 64 where n is the number of images downloded. The shape of the flatten images should be (1797, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "flatten_images = digits.images.reshape((len(digits.images), -1))\n",
    "\n",
    "print(flatten_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing k-means\n",
    "We note that, as the images are labeled, any supervised learning algorithm can be applied to this collection of images. However, our objective is to see if we can use unsupervided learning methods such as k-means to cluster images (w/o considering their labels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans++\n",
    "The original k-means algorithm choose cluster centers at random. However, kmeans++ allows the selection of initial centers away from each other to avoid cluster centers converging together and loss converging to a local minima. \n",
    "The algorithm for kmeans++ (selecting initial cluster centers) is given by\n",
    "\n",
    "<img src=\"kmeans-pp.jpg\" alt=\"Alt text\" height=\"100\" width=\"300\">\n",
    "\n",
    "GIven below is a sample implementation of kmeans++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans_plus(X, k):\n",
    "    # Select the first cluster center randomly\n",
    "    centers = [X[np.random.choice(X.shape[0], 1, replace=False)][0]]\n",
    "\n",
    "    # Select the remaining cluster centers using k-means++\n",
    "    for i in range(1, k):\n",
    "        # Compute the distances to the nearest cluster center for each data point\n",
    "        distances = np.min(np.sum((X - centers[-1])**2, axis=1)[:, np.newaxis], axis=1)\n",
    "\n",
    "        # Compute the probabilities of each data point being selected as the next cluster center\n",
    "        probs = distances / np.sum(distances)\n",
    "\n",
    "        # Select the next cluster center randomly, weighted by the probabilities\n",
    "        centers.append(X[np.random.choice(X.shape[0], 1, replace=False, p=probs)][0])\n",
    "\n",
    "    return np.array(centers)\n",
    "Mu = kmeans_plus(flatten_images, 3)   \n",
    "Mu   # print the initial centers with k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The complexity of the kmeans++ code above is O(k\\*n\\*m). The complexity can be found by first  looping through the additional clusters k-1 times (aside from the first one, which is O(1)). The complexity is thus O(n) since the distances are being calculated in an n-dimensional space. Over m data points, the complexity becomes O(n\\*m).  O(m) also represents the sum of the distances. \n",
    "\n",
    "> O(1) + (k-1) * \\[(O(m\\*n) + O(m) + O(1)\\] = O(k\\*n\\*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is kmeans++ always better than random? \n",
    "In original k-means algorithm, we choose initial centers at random. This can lead to centers that will make error converging to local minimas. Here, we are exploring if the idea of using kmeans++ is a better way to choose the initial centers than choosing them at random.\n",
    "\n",
    "We will test this with the 2D data set  [1, 1], [0, 1], [1, 0], [2, 1], [1, 2], [3, 2], [2, 3], [4, 10], [10, 4], [5, 5] of 10 points. Define the error as the sum of the manhattan distances of each point from the \"closest' centers. Run the random algorithm t iterations (t=10, 15, 25, 50, 100) and record total errors in each case. For each t, print the probability that k-means++ choose centers better than the random algorithm? It is possible that random selection can sometimes work better than kmeans++. Run a few times to form an opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manhattan distance for 2D vectors\n",
    "def manhattan(x, y):\n",
    "    return (abs(x[0]-y[0]) + abs(x[1]-y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 1], [0, 1], [1, 0], [2, 1], [1, 2], [3, 2], [2, 3], [4, 10], [10, 4], [5, 5]])\n",
    "n = 10   # num points\n",
    "list_iter = [10, 15, 20, 25, 50, 100]   # num iterations list\n",
    "k = 2 # given\n",
    "\n",
    "\n",
    "\n",
    "def rand(x, k):\n",
    "    indices = np.random.choice(x.shape[0], k, replace=False)\n",
    "    return x[indices]\n",
    "\n",
    "def errors(x, centers):\n",
    "    error = 0\n",
    "\n",
    "    for point in x:\n",
    "        distances = [manhattan(point, center) for center in centers]\n",
    "        error += min(distances)\n",
    "    return error\n",
    "\n",
    "for i in list_iter:\n",
    "    kmeans_pro = 0\n",
    "    \n",
    "    for j in range(i):\n",
    "        k_center = kmeans_plus(x, k)\n",
    "        r_center = rand(x, k)\n",
    "        \n",
    "        k_error = errors(x, k_center)\n",
    "        r_error = errors(x, r_center)\n",
    "        \n",
    "        if k_error < r_error:\n",
    "            kmeans_pro += 1\n",
    "            \n",
    "    c = kmeans_pro / i\n",
    "    \n",
    "    print(f\"Over {i} iterations there is a {c} chance that k-means++ beats random.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thus, on average, k-means++ is better than random variable creation for selecting initial cluster centers. Thus, the clustering is more precise, and using k-means++ to choose centers is a better idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing k-means algorithm using Matrix forms\n",
    "\n",
    "Max iterations of 100 or until the difference between prev_error and current_error is less than $10^{-5}$ whichever comes first. The function will return the k-centers (Mu) and loss. Loss generally will go down as you increase k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the kmeans algorithm with random initial points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k = 10\n",
    "\n",
    "\n",
    "def kmeans_matrix(x, k):\n",
    "    rows, cols = x.shape\n",
    "    mu = x[np.random.choice(rows, k, replace=False)]\n",
    "\n",
    "    for i in range(100):\n",
    "        d = np.sqrt(((x[:, np.newaxis, :] - mu[np.newaxis, :, :]) ** 2).sum(axis=2)) # find euclidean distance to all centers\n",
    "        cluster = np.argmin(d, axis=1)\n",
    "        temp = np.array([x[cluster == j].mean(axis=0) for j in range(k)]) #calculate clusters\n",
    "        \n",
    "        if np.allclose(mu, temp): #break if converging\n",
    "            break\n",
    "\n",
    "        mu = temp\n",
    "        \n",
    "    loss = np.sum((x - mu[cluster]) ** 2)\n",
    "    return mu, loss\n",
    "\n",
    "for i in range(10, 31):\n",
    "    mu, loss = kmeans_matrix(flatten_images, i)\n",
    "    print('k=', i, ' Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the kmeans algorithm with kmeans++ initial points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k = 10\n",
    "\n",
    "\n",
    "def kmeans_plus(x, k):\n",
    "    centers = [x[np.random.choice(x.shape[0], 1, replace=False)][0]] # random center\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        dist = np.min(np.array([np.sum((x - center) ** 2, axis=1) for center in centers]), axis=0) # min distance to centers\n",
    "        centers.append(x[np.random.choice(x.shape[0], 1, replace=False, p=dist / np.sum(dist))][0]) # select and add new center\n",
    "    return np.array(centers)\n",
    "\n",
    "def kmeans_matrix(x, k):\n",
    "    rows, cols = x.shape\n",
    "    mu = kmeans_plus(x, k)\n",
    "\n",
    "    for iter in range(100):\n",
    "        d = np.sqrt(((x[:, np.newaxis, :] - mu[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        cluster = np.argmin(d, axis=1)\n",
    "        temp = np.array([x[cluster == j].mean(axis=0) for j in range(k)])\n",
    "        if np.allclose(mu, temp):\n",
    "            break\n",
    "        mu = temp\n",
    "        \n",
    "    loss = np.sum((x - mu[cluster]) ** 2)\n",
    "    return mu, loss\n",
    "\n",
    "for i in range(10,31):\n",
    "    mu, loss = kmeans_matrix(flatten_images, i)\n",
    "    print('k=', i, ' Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Overall, it is seen that k_means++ has a lower average loss across different values of k. This shows that k_means++ is better for starting the algorithm, which could mean more optimized clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Also, as k increases, the loss decreases since having more clusters means the data's structure is used more accurately. However, the decrease is not consistent, and sometimes the loss does increase as k increases. This means that there is a point where adding more clusters does not result in better perforamnce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate implementation of k-means\n",
    "Given below is an alternate implementation of k-means to matrix implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans_alt(X, k, max_iter=100):\n",
    "    \n",
    "    # randomly initialize k centroids\n",
    "    Mu = X[np.random.choice(X.shape[0], size=k, replace=False)]\n",
    "    # choose initial means using kmeans++\n",
    "    #Mu = kmeans_pp(flatten_images, k)\n",
    "    prev_loss = np.inf\n",
    "    for i in range(max_iter):\n",
    "        # assign each data point to the nearest centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis, :] - Mu, axis=2)\n",
    "        cluster_assignment = np.argmin(distances, axis=1)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = np.sum(np.square(np.min(distances, axis=1)))\n",
    "        \n",
    "        # recompute centroids as the mean of the assigned data points\n",
    "        for j in range(k):\n",
    "            Mu[j] = np.mean(X[cluster_assignment == j], axis=0)\n",
    "\n",
    "        # check for convergence\n",
    "        if np.abs(prev_loss - loss) < 1e-5:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "    return cluster_assignment, Mu, loss\n",
    "\n",
    "for i in range(10,31):\n",
    "    cluster_assignment, Mu, loss = kmeans_alt(flatten_images, i)\n",
    "    print('k=', i, ' ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images in a Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in the collection will be placed into one of the k clusters. We need to find all images in the cluster. We should see that the majority of the images in a cluster are quite similar; we can verify this by printing the dominant image in the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from collections import Counter\n",
    "j = 12   # cluster number\n",
    "\n",
    "\n",
    "\n",
    "k = 15\n",
    "mu, _ = kmeans_matrix(flatten_images, k)\n",
    "\n",
    "d = np.sqrt(((flatten_images[:, np.newaxis, :] - mu[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "cluster = np.argmin(d, axis=1)\n",
    "i = np.where(cluster == j)[0]\n",
    "centroid = np.sqrt(np.sum((flatten_images[i] - mu[j]) ** 2, axis=1))\n",
    "\n",
    "print(\"Cluster image labels:\", j, digits.target[i])\n",
    "plt.imshow(digits.images[i[np.argmin(centroid)]])\n",
    "plt.title(f\"Cluster {j} (Index: {i[np.argmin(centroid)]})\")\n",
    "\n",
    "plt.show() # dominant image\n",
    "\n",
    "fig, axes = plt.subplots(1, min(len(i), 10), figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images[i], digits.target[i]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(label)\n",
    "plt.show() # images in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error rate\n",
    "Here, we will actually use the original labels of the images to see error rate in a specific cluster. We define the error rate as the ratio of **(all digits - number_of_dominant_digit_in_the cluster)/ all digits** in the cluster. For example, if the cluter had a total of 100 elements and 80 of them were the same digit (say 2), the the error rate is (100-80)/100 = 0.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "k = 20\n",
    "Mu, _ = kmeans_matrix(flatten_images, k)\n",
    "d = np.sqrt(((flatten_images[:, np.newaxis, :] - Mu[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "cluster = np.argmin(d, axis=1)\n",
    "\n",
    "for j in range(min(k, 20)):\n",
    "    label = digits.target[np.where(cluster == j)[0]]\n",
    "    most_common_digit, n = Counter(label).most_common(1)[0] # n = most common cluster\n",
    "    error = (len(label) - n) / len(label) # (all digits - number_of_dominant_digit_in_the cluster) / all digits\n",
    "\n",
    "    print(f\"Cluster {j} - Dominant Digit = {most_common_digit}, Error = {error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best predicted Clusters\n",
    "If the error rate of a specific cluster is almost zero, then they are the best predicted clusters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The best predicted clusters are the ones with a near-zero error rate, e.g. closers 0, 3, 9, 13, 15, 18. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Clusters\n",
    "One of the most challenging part of clustering is to determine the minimum number of clusters required to solve the problem. In this case, we know, ideally we need 10 clusters. But with k=10, we may not get a unique dominant integer in a each cluster. So In this part, we will explore ways to determine the minimum value of k, that can give us k clusters that contains all digits 0..9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a minimum k \n",
    "It may be the case that if we go with k=10, we may not find that 10 cluster centers that do represent the digits from 0...9. Instead we may have to increase k, in order to find a k where each center is now representing one of 0...9. Here, we need to find the minimum k, where each cluster will be dominated by one of 0..9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def first_index_with_all_integers(arr):\n",
    "    \n",
    "    for k in range(10, 21):\n",
    "        mu, _ = kmeans_matrix(arr, k)\n",
    "        d = np.sqrt(((arr[:, np.newaxis, :] - mu[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        cluster = np.argmin(d, axis=1)\n",
    "        represented = set()\n",
    "        \n",
    "        for j in range(k):\n",
    "            i = np.where(cluster == j)[0]\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            res, _ = Counter(digits.target[i]).most_common(1)[0] # res = most common digit\n",
    "            represented.add(res)\n",
    "\n",
    "        if len(represented) == 10:\n",
    "            return k \n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"Minimum k:\", first_index_with_all_integers(flatten_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the minimum k found above, printed below are the cluster centers as 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 14\n",
    "mu, _ = kmeans_matrix(flatten_images, k)\n",
    "\n",
    "fig, axes = plt.subplots(1, k, figsize=(k, 1.5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(Mu[i].reshape(8, 8))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'cluster {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> since there are only 14 clusters, some digits are combined into the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a good k by plotting\n",
    "Finding a k is often context dependent. Ideally, in this case, we would like to group handwritten digits into 10 clusters. However, we may need more than 10 clusters to capture all digits. So, we will plot the k vs loss to see if we can get some idea about the minimum k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding vectors loss and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=[]\n",
    "y=[]\n",
    "print('k            loss')\n",
    "for i in range(10,30):\n",
    "    cluster_assignment, mu, loss = kmeans_alt(flatten_images, i)\n",
    "    x.append(i)\n",
    "    y.append(loss)\n",
    "    print('k=', i, ' ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss vs k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use x and y vectors from previous part\n",
    "x = x\n",
    "y = y\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, color='blue', label='Datapoints') \n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal k-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just based on the loss, a k-value of around 29 is ideal. The loss begins to reach a limit, showing that it is not worth reducing the loss by using additional clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence and timing kmeans with matrix implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "res = []\n",
    "\n",
    "for k in range(10, 31):\n",
    "    start_time = time.time()\n",
    "    mu, loss = kmeans_matrix(flatten_images, k)\n",
    "    end_time = time.time()\n",
    "    res.append((k, round(loss), end_time - start_time))\n",
    "    \n",
    "for result in res:\n",
    "    print(\"When k = {:2}, loss ={:10} and time elapsed = {:.6f}s\".format(*result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence and Timing kmeans with kmeans_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "res2 = []\n",
    "\n",
    "for k in range(10, 31):\n",
    "    start_time = time.time()\n",
    "    cluster_assignment, mu, loss = kmeans_alt(flatten_images, k)\n",
    "    end_time = time.time()\n",
    "    res2.append((k, round(loss) , end_time - start_time))\n",
    "\n",
    "for result in res2:\n",
    "    print(\"When k = {:2}, loss ={:10} and time elapsed = {:.6f}s\".format(*result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data size vs time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "k=18\n",
    "m = []\n",
    "t = []\n",
    "m_values = range(100, 1100, 100)\n",
    "times = []\n",
    "\n",
    "\n",
    "\n",
    "for m in m_values:\n",
    "    i = random.sample(range(len(flatten_images)), m)\n",
    "    start_time = time.time()\n",
    "    mu, loss = kmeans_matrix(flatten_images[i], k)\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "print(\"Data Size:\", list(m_values))\n",
    "print(\"Time elapsed:\", times)\n",
    "# print(m)\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use m and t vectors from previous part\n",
    "x = m\n",
    "y = t\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(m_values, times)\n",
    "plt.xlabel('Data Size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The graph shows that there is a slightly linear increase in the amount of time it takes to run the k-means algorithm. Every iteration of the algorithm, the distance between datapoints and clusters is calculated and the time complexity increases linearly. This confirms the complexity analysis, O(m\\*n\\*t\\*k), of 2.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
